<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TextureSAM: Towards a Texture Aware Foundation Model for Segmentation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
	    tex2jax: {
	        inlineMath: [['$','$'], ['\\(','\\)']],
	        processEscapes: true
	    }
	});
    </script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="xtitle is-1 publication-title">TextureSAM: Towards a Texture Aware Foundation Model for Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors in a single line -->
              <span class="author-block"><strong>Inbal Cohen<sup>*</sup></strong>,</span>
              <span class="author-block"><strong>Boaz Meivar<sup>*</sup></strong>,</span>
              <span class="author-block">Peihan Tu,</span>
              <span class="author-block">Shai Avidan,</span>
              <span class="author-block">Gal Oren</span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block">Institution Name · Conference Name and Year</span>
              <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
            </div>
          </div>
        </div>
      </div>
    </div>
</section>




<!-- Paper abstract -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ADE20K Image Carousel</title>

    <!-- Slick Carousel CSS -->
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/slick-carousel/slick/slick.css"/>
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/slick-carousel/slick/slick-theme.css"/>

    <style>
        /* Center carousel */
        .carousel-container {
            max-width: 1000px;
            margin: auto;
            padding: 10px;
        }

        /* Ensure images are the same size and have spacing */
        .slick-slide {
            margin: 0 10px; /* Adds space between images */
        }

        .slick-slide img {
            width: 300px; /* Fixed width */
            height: auto; /* Maintain aspect ratio */
            display: block;
            margin: auto;
        }

        /* Adjust navigation arrows */
        .slick-prev, .slick-next {
            z-index: 1000;
        }

        /* Style navigation dots */
        .slick-dots {
            bottom: -30px;
        }

        .subtitle {
            text-align: center;
            margin-top: 5px;
        }
    </style>
</head>
<body>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Segment Anything Models (SAM) have achieved remarkable success in object segmentation tasks across diverse datasets. However, these models are predominantly trained on large-scale semantic segmentation datasets, which introduce a bias toward object shape rather than texture cues in the image. This limitation is critical in domains such as medical imaging, material classification, and remote sensing, where texture changes define object boundaries. In this study, we investigate SAM’s bias toward semantics over textures and introduce a new texture-aware foundation model, TextureSAM, which performs superior segmentation in texture-dominant scenarios. To achieve this, we employ a novel fine-tuning approach that incorporates texture augmentation techniques, incrementally modifying training images to emphasize texture features. By leveraging a novel texture-alternation of the ADE20K dataset, we guide TextureSAM to prioritize texture-defined regions, thereby mitigating the inherent shape bias present in the original SAM model. Our extensive experiments demonstrate that TextureSAM significantly outperforms SAM on both natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation datasets. The code and texture-augmented dataset will be publicly available.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- ADE20K Image Carousel -->
<section class="hero is-small">
    <div class="hero-body" style="padding: 1rem 0;">
        <div class="container">
            <h2 class="title is-3 has-text-centered">ADE20K Dataset: Texture Variations</h2>
            
            <!-- Added Caption Below the Title -->
            <p class="has-text-centered" style="margin-bottom: 15px; font-size: 1.1rem; color: #4a4a4a;">
                Gradual texture shifts in ADE20K images as η varies from 0 to 1, preserving semantics at low values and fully transforming textures at η = 1.
            </p>

            <div class="carousel-container">
                <div id="ade20k-carousel">
                    
                    <div><img src="static/images/ADE_train_00005115.jpg" alt="Real Image"/><h2 class="subtitle">Real Image</h2></div>
                    <div><img src="static/images/textured_degree0.0_v2.jpg" alt="Texture 0.0"/><h2 class="subtitle">Textured Image (η = 0.0)</h2></div>
                    <div><img src="static/images/textured_degree0.1_v2.jpg" alt="Texture 0.1"/><h2 class="subtitle">Textured Image (η = 0.1)</h2></div>
                    <div><img src="static/images/textured_degree0.2_v2.jpg" alt="Texture 0.2"/><h2 class="subtitle">Textured Image (η = 0.2)</h2></div>
                    <div><img src="static/images/textured_degree0.3_v2.jpg" alt="Texture 0.3"/><h2 class="subtitle">Textured Image (η = 0.3)</h2></div>
                    <div><img src="static/images/textured_degree0.4_v2.jpg" alt="Texture 0.4"/><h2 class="subtitle">Textured Image (η = 0.4)</h2></div>
                    <div><img src="static/images/textured_degree0.5_v2.jpg" alt="Texture 0.5"/><h2 class="subtitle">Textured Image (η = 0.5)</h2></div>
                    <div><img src="static/images/textured_degree0.6_v2.jpg" alt="Texture 0.6"/><h2 class="subtitle">Textured Image (η = 0.6)</h2></div>
                    <div><img src="static/images/textured_degree0.7_v2.jpg" alt="Texture 0.7"/><h2 class="subtitle">Textured Image (η = 0.7)</h2></div>
                    <div><img src="static/images/textured_degree0.8_v2.jpg" alt="Texture 0.8"/><h2 class="subtitle">Textured Image (η = 0.8)</h2></div>
                    <div><img src="static/images/textured_degree0.9_v2.jpg" alt="Texture 0.9"/><h2 class="subtitle">Textured Image (η = 0.9)</h2></div>
                    <div><img src="static/images/textured_degree1.0_v2.jpg" alt="Texture 1.0"/><h2 class="subtitle">Textured Image (η = 1.0)</h2></div>
                    <div><img src="static/images/ADE_train_00005115_seg.png" alt="Segmentation Map"/><h2 class="subtitle">Segmentation Map</h2></div>

                </div>
            </div>

        </div>
    </div>
</section>
<!-- End ADE20K Image Carousel -->

<section class="hero is-light">
  <div class="hero-body" style="padding: 2rem 0;">
      <div class="container has-text-centered">
          <h2 class="title is-3">Illustration of Texture-Based Image Generation</h2>
          <figure>
              <img src="static/images/ADE20K_figure.png" alt="Texture Image Generation Process" style="width: 70%; max-width: 700px; display: block; margin: auto;">
              <figcaption style="margin-top: 15px; font-size: 1rem; color: #4a4a4a;">
                  Illustration of generating textured images for dataset augmentation using a compositional neural texture (CNT) approach. 
                  SAM-2 couples shape and texture cues, expecting specific textures for certain objects. 
                  To decouple these, we use ground truth masks from ADE20K to generate arbitrary textures, 
                  enabling the fine-tuning of SAM-2 into TextureSAM. This enhanced dataset improves segmentation 
                  by reducing texture bias while preserving structure.
              </figcaption>
          </figure>
      </div>
  </div>
</section>

<!-- jQuery and Slick Carousel Scripts -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/slick-carousel/slick/slick.min.js"></script>

<script>
    $(document).ready(function(){
        $('#ade20k-carousel').slick({
            slidesToShow: 3,  // Ensures 3 images are visible at a time
            slidesToScroll: 3, // Moves 3 images at a time
            dots: true,
            infinite: true,
            autoplay: false,
            arrows: true,
            responsive: [
                {
                    breakpoint: 1024,
                    settings: {
                        slidesToShow: 3,
                        slidesToScroll: 3
                    }
                },
                {
                    breakpoint: 768,
                    settings: {
                        slidesToShow: 2,
                        slidesToScroll: 2
                    }
                },
                {
                    breakpoint: 480,
                    settings: {
                        slidesToShow: 1,
                        slidesToScroll: 1
                    }
                }
            ]
        });
    });
</script>

</body>
</html>



<!-- jQuery and Slick Carousel Scripts -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/slick-carousel/slick/slick.min.js"></script>

<script>
    $(document).ready(function(){
        $('#ade20k-carousel').slick({
            slidesToShow: 3,  // Number of images visible at a time
            slidesToScroll: 1, // Number of images to scroll at a time
            dots: true,
            infinite: true,
            autoplay: false,
            arrows: true,
            responsive: [
                {
                    breakpoint: 1024,
                    settings: {
                        slidesToShow: 3
                    }
                },
                {
                    breakpoint: 768,
                    settings: {
                        slidesToShow: 2
                    }
                },
                {
                    breakpoint: 480,
                    settings: {
                        slidesToShow: 1
                    }
                }
            ]
        });
    });
</script>

</body>
</html>



<!-- Results Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <p>
            We evaluated TextureSAM across multiple datasets, comparing it to SAM-2 and SAM-2*. SAM-2 is the original SAM variant, relying on shape cues, while SAM-2* is a modified version using TextureSAM’s inference settings to reduce shape bias. The results below highlight TextureSAM’s improved texture-based segmentation.
          </p>
          
          <!-- TextureSAM on Real-World Data (Now First) -->
          <div class="has-text-centered" style="margin-bottom: 40px; display: flex; justify-content: center; align-items: center; flex-direction: column;">
            <figure style="text-align: center;">
              <img src="static/images/TextureSAM.png" alt="RWTD Segmentation Results" style="width: 100%; max-width: 1100px; display: block; margin: auto;">
              <figcaption style="text-align: center;">
                Examples for segmentation of natural images from the Real-World Textured Dataset. Compared with the original SAM (2nd and
                3rd rows, SAM-2* indicating modified inference parameters), TextureSAM (4th row) is more adept at recognizing areas defined by texture
                changes. SAM’s reliance on semantic shape results in fragmented segmentation, while TextureSAM provides segmentation of whole areas
                defined by texture changes.
              </figcaption>
              <p><strong>Dataset: RWTD (Real-World Textured Dataset)</strong></p>
            </figure>
          </div>
          


          <!-- STMD Results -->
          <div class="has-text-centered" style="margin-bottom: 40px;">
            <figure>
              <img src="static/images/Results_images.png" alt="ADE20K and STMD Segmentation Results" style="width: 100%; max-width: 1100px;">
              <figcaption>
                Segmentation results on ADE20K (left) and STMD (right). 
                TextureSAM (3rd row) produces comparable semantic segmentation to SAM-2 (2nd row, 3rd row with modified inference). 
                TextureSAM’s predictions align better with GT, preserving entire textured regions (e.g., trees, walls). 
                On the STMD dataset, TextureSAM maps better to GT, while SAM-2 fragments textures into smaller elements.
              </figcaption>
              <p><strong>Dataset: STMD (Synthetic Texture-based Material Dataset)</strong></p>
            </figure>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Results Section -->




<!-- Paper poster 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/vinthony/project-page-template">modification version</a> of <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">TextureSAM: Towards a Texture Aware Foundation Model for Segmentation Template</a> from <a href="https://github.com/vinthony">vinthony</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>