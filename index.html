<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TextureSAM: Towards a Texture Aware Foundation Model for Segmentation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({
	    tex2jax: {
	        inlineMath: [['$','$'], ['\\(','\\)']],
	        processEscapes: true
	    }
	});
    </script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="xtitle is-1 publication-title">TextureSAM: Towards a Texture Aware Foundation Model for Segmentation</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup></span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup></span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link ">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link ">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link ">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link ">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<<<<<<< HEAD
<section class="hero teaser">
  <div class="container is-max-desktop">
<div class="marquee-wrapper">
	<div class="container">
		<div class="marquee-block">
			<div class="marquee-inner to-left">
				<span>
					<div class="marquee-item">
            <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">1</p>
					</div>
					<div class="marquee-item">
            <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">2</p>
					</div>
					<div class="marquee-item">
            <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">3</p>
					</div>
					<div class="marquee-item">
            <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">4</p>
					</div>
					<div class="marquee-item">
            <img src="static/images/carousel5.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">5</p>
					</div>
				</span>
				<span>
					<div class="marquee-item">
            <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">1</p>
					</div>
					<div class="marquee-item">
            <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">2</p>
					</div>
					<div class="marquee-item">
            <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">3</p>
					</div>
					<div class="marquee-item">
            <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">4</p>
					</div>
					<div class="marquee-item">
            <img src="static/images/carousel5.jpg" alt="MY ALT TEXT"/>
						<p class="text-white">5</p>
					</div>
				</span>
			</div>
		</div>

  </div>
</section>
=======
>>>>>>> 2334507 (Updated index.html and added images for results section)

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           Segment Anything Models (SAM) have achieved remarkable success in object segmentation tasks across diverse datasets. However, these models are predominantly trained on large-scale semantic segmentation datasets, which introduce a bias toward object shape rather than texture cues in the image. This limitation is critical in domains such as medical imaging, material classification, and remote sensing, where texture changes define object boundaries. 
            In this study, we investigate SAMâ€™s bias toward semantics over textures and introduce a new texture-aware foundation model, TextureSAM, which performs superior segmentation in texture-dominant scenarios. To achieve this, we employ a novel fine-tuning approach that incorporates texture augmentation techniques, incrementally modifying training images to emphasize texture features. By leveraging a texture-altered version of the ADE20K dataset, we guide TextureSAM to prioritize texture-defined regions, thereby mitigating the inherent shape bias present in the original SAM model. 
            Our extensive experiments demonstrate that TextureSAM significantly outperforms SAM-2 on both natural (+0.2 mIoU) and synthetic (+0.18 mIoU) texture-based segmentation datasets. The code and texture-augmented dataset will be publicly available. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Results Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <p>
            We evaluated TextureSAM on multiple datasets, comparing its segmentation performance to the original SAM-2 model. The results below demonstrate how TextureSAM improves texture-based segmentation by reducing fragmentation and aligning better with ground truth annotations.
          </p>

<<<<<<< HEAD
=======
          <!-- Augmentation Illustration -->
          <div class="has-text-centered">
            <figure>
              <embed src="static/images/augmentation.pdf" type="application/pdf" width="100%" height="600px">
              <figcaption>
                Illustration of generating textured images for dataset augmentation using [28]. This process was used to create the augmented ADE20K dataset, enhancing texture-based segmentation performance.
              </figcaption>
            </figure>
          </div>

          
          <!-- ADE20K Results -->
          <div class="has-text-centered">
            <figure>
              <img src="static/images/ADE20K.png" alt="ADE20K Segmentation Results" style="width: 100%; max-width: 900px;">
              <figcaption>
                TextureSAM achieves comparable segmentation to SAM-2 but provides better consistency in texture-rich areas such as trees and walls. Unlike SAM-2, which may fragment textured regions into multiple segments, TextureSAM ensures that entire regions with similar textures are marked as a single instance, aligning more closely with the ground truth.
              </figcaption>
            </figure>
          </div>

          <!-- STMD Results -->
          <div class="has-text-centered">
            <figure>
              <img src="static/images/STMD.png" alt="STMD Segmentation Results" style="width: 100%; max-width: 900px;">
              <figcaption>
                On the synthetic STMD dataset, TextureSAM significantly improves segmentation performance compared to SAM-2. The model trained with strong texture augmentations achieves the highest mean Intersection over Union (mIoU) and Adjusted Rand Index (ARI), outperforming both SAM-2 and the version of TextureSAM with mild augmentations. SAM-2 struggles with segmenting non-salient, texture-rich regions, often producing sparse coverage, whereas TextureSAM provides more cohesive segmentations.
              </figcaption>
              <p><strong>Dataset: STMD (Synthetic Texture-based Material Dataset)</strong></p>
            </figure>
          </div>

          <!-- TextureSAM on Real-World Data -->
          <div class="has-text-centered">
            <figure>
              <img src="static/images/TextureSAM.png" alt="RWTD Segmentation Results" style="width: 100%; max-width: 900px;">
              <figcaption>
                On real-world textured images, TextureSAM consistently outperforms SAM-2 by recognizing texture-defined areas more effectively. While SAM-2 relies heavily on semantic object shapes, leading to fragmented segmentations, TextureSAM adapts to texture variations and produces segmentation maps that better match the expected ground truth regions.
              </figcaption>
              <p><strong>Dataset: RWTD (Real-World Textured Dataset)</strong></p>
            </figure>
          </div>
          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Results Section -->



>>>>>>> 2334507 (Updated index.html and added images for results section)



<!-- Paper poster 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/vinthony/project-page-template">modification version</a> of <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">TextureSAM: Towards a Texture Aware Foundation Model for Segmentation Template</a> from <a href="https://github.com/vinthony">vinthony</a>.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>